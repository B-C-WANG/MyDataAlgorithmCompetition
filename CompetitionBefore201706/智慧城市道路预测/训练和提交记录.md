
---
- 实验记录，第一次使用batch normalization，不加激活函数，loss从1.5持续下降到1.2，很慢，但是很稳定
-  第二次添加tanh激活函数，loss跳跃很大，从1.7到1.0左右，很快下降loss，但是一会又很快上升
-  第三次，取消batch normalization，下降很慢但很稳定。
-  之后经过了batch_size1000，epoch1000多次训练使loss从1.5降到1，**平均绝对差值为7左右，平均相对误差为0.3**。
- 2017年7月28日采用的模型：![](http://i.imgur.com/Ju7rrjK.png)，其中预处理是x_train和y_train一起fit正则化，test也是，` model.compile(loss='mean_squared_error', optimizer="adadelta")`，可能test采用train的正则化会有问题。
- 至此，进行了第一次predict，提交了结果**2017年7月28日.csv**


---
- **2017年7月29日**，由于2017年7月28日的时间不合理（比如08:58:00-08:60:00要改成-09:00:00），所以没有成绩，准备今天仍然提交昨天的结果，保证格式正确，模型和前一天的一样，上传后得到的loss为**0.653916**，而在训练集上的loss约为**0.3**左右，推测有过拟合现象，准备将数据集划分，划分成x_train和x_test等进行处理，同时稍微改变模型结构重新训练。

---
- **2017年7月30日**，划分数据集为训练集和测试集，测试集比例约为0.3。这样能够同时看到训练集和测试集的loss情况，推测合适的训练时间，另外loss下降很慢，需要对模型进行修改。**在重新训练前，原来的模型valid loss为1.01左右，并且还会随着train loss的下降而上升，正在过拟合**
- 目前暂定的修改有**增加前两层filters数量为20，最后一层kernel_size第3维由3变成20**。重新建立模型训练后，一开始的loss为（默认前一个是train loss，后一个是valid loss）[1.6,1.65]。batch_size1000，epoch100后，train loss由1.5下降为1.3，但是valid loss从1.65下降到1.55并且没有收敛的迹象，需要采用方法防止过拟合
- **我认为kernel size的大小应该是没有问题的，就应该很大，但是loss降不下来的原因，可能是模型存在问题**
- **之后，启用batch_normalization进行，并在前两层之前加上0.25的dropout，一定程度上防止过拟合，并启用tensorboard观察**
- **效果不好！使用单纯的CNN进行，使用CNN而不使用LSTM的话，一开始的loss在2.0,1.9左右。**此时将kernel_size从60，132改成**3,3**，相当于直接用**图像**的方式来对待，如果能够训练到聪明到自己找到节点关系的话，那么loss应该下降很稳定。**结果：训练很快，loss从2.1,1.8到2.0,1.75**。之后继续训练，loss从一开始的2.0,1.7下降到1.6,1.6之后，训练集loss仍然在下降，但是测试集的loss保持在1.6没有变动。**这个模型的loss没有ConvLSTM好，但是过拟合情况还好**。结果是，x_test数据集上平均相对误差是（预测结果与真实差值比上真实值的平均）0.7，所有数据集平均误差也为0.7左右。**比起ConvLSTM效果很差，如果至少达到LSTM的效果，需要valid loss低于1.0**。
- 之后增加了模型的深度，kernelsize从全部是3,3变成60,60,30，30,15,15这几种，结果发现一开始的loss很大，后来训练很慢，train的loss能够下降到。1.3，valid的loss为1.56。至此，模型如下：![](http://i.imgur.com/1Sl8jfu.png)
- 接着将kernel size全部换为6,6，采用深度模型，dropout全部定为0.3。这样的模型以100的batch训练1000个epoch后，loss从2.1下降到1.4，validloss从1.8下降到1.6就没有变动了，最终的误差（**以后都指预测结果与真实差值比上真实值的平均**）都是在0.68左右。
- 将dropout设置为0.5，增大batch到1000，epoch1000，进行一次大的训练（之前ConvLSTM都是训练了1h之后，达到validloss为1.0以下），采用conv而不是conLSTM的好处是，之后复赛容易在平台上实现。**经过这次加大dropout，1000batch和epoch之后**，loss还是不能够下降，维持在1.4，1.5左右，但是**突然发现，kernelsize小的时候，参数数量也小，说明参数数量可能不够，于是增加一层，然后增大kernelsize，其中第一层kernelsize为60,132，filters数量设为10，然后2-3层，filter提升10倍变成100，kernelsize变成12,12**
- <font color="red">*应当注意参数数量要足够！比较参数数量很重要，之前一直都忽视了*</font>
- 训练之后发现很慢，看参数个数的时候发现，**中间层的参数较多，但实际上我们想要第一层参数最多，因为这个数据相关性不是很深，所以尽量在前面几层参数多一点儿，所以增加了一开始filter数量，减少了中间层的filter数量**。
- 但在此训练之前，我调整了参数使其训练加快，因为突然发现，**首要任务是降低valid loss而不是对抗过拟合，尽管这次我将dropout设置为了0.75，**
- 经过1000,1000（**也就是batch_size=1000，epoch=1000**）之后，得到的结果为


- <font color=green>对此我也有了一点儿经验：如果参数多，loss训练慢，但是可能达到很小的loss，能够充分拟合。在完全loss稳定前，不要轻易下结论。**数据集的数量与神经元参数的数量应当存在一定的最佳比例关系，这样训练才会更快更好，又不浪费。**考虑了训练集loss之后，再考虑一下过拟合的问题。</font>
- **实际上CNN效果没有ConvLSTM好，所以打算专门训练ConvLSTM，之后复赛再看看CNN**
- **更改原来的ConvLSTM结构：参考CNN结构，其实kernel_size不用设置太大，首先训练一个参数小的模型，看能不能够满足要求，毕竟数据集并不是很大。**
- **令人惊讶的是，ConvLSTM即使采用100，10的参数训练10秒，其得到的误差也只有0.49！另外我将第一层的filter设置为1，大小为132，因为只要进行一次预处理包含节点关系就够了，比如我们可以将节点关系转化成132 x 132的矩阵，将上游节点的关系用加权的方式进行修正**
- 训练过后，valid loss没有怎样下降，但是train的loss下降很快，为了保证不过拟合，将所有的dropout设置为0.9！**似乎有奇效，valid loss迅速降低了，然而训练到最后，反而得到的误差很大，将近0.6左右，是drop out 过高，还是网络参数不够大？**
- 这次将dropout设置为0.5，然后网络参数增加来进行92,500的训练，此次参数总量为50000左右，**注意每次训练epoch不要太高，这样能够快速看到预测结果误差**
- 接下来，不考虑valid loss，因为发现即使valid loss很高，最后的误差也是有所下降的，所以一直训练下去，降低train loss
- **但是最后训练完发现，误差竟然上升了！直接采用绝对误差作为loss！（反正正则化），名称是loss="mean_absolute_error"**
- 很奇怪，采用mean_absolute_percentage_error，一开始loss是900，100，后来变成300,300，然后才变成200,200，非常奇怪的走势。于是我打算在epoch不同值的时候停止，然后验证结果。
- 结果却显示，无论训练多少次，最终的误差，总是在0.45以上，而且越是训练，越是误差增大，由此，可能我需要更换到一开始采用的模型当中，重新思考一下，或者，再次采用CNN算法，使用另一种loss
- **最后我发现，CNN一开始的结果误差就是0.6，而且越训练，误差越大**
- <font color="red">于是我产生了怀疑，为什么越训练越不准确？难道说根本不应该训练？是**optimizer有问题？**，可能**模型一开始就决定了误差，训练可能需要找其他方法。**于是我打算找回上次训练之后得到0.3误差的那一组条件</font>
- 不过在此之前，我先将batch_normalization禁用试一试。结果是loss很低了，但是误差仍然很大。后来尝试把dropout删掉，无效
- 换了新的模型之后，仍然一开始误差是0.45左右，由此**我决定在每10个epoch中进行一次预测，显示误差结果**
- **当使用mean_absolute_percentage_error时，误差从0.45开始上升，当采用mean_squared_error时，误差一开始是0.6，并逐渐上升，然后开始逐渐下降。所以，选择loss函数需要进行严格的考虑，查阅资料进行。采用poisson作为loss函数，误差为0.7。"mean_squared_logarithmic_error"，误差为0.6。"mean_absolute_error"，一开始是0.44，似乎有随着训练而下降的趋势！但是也会有一些摇摆。之后选择优化器，似乎adam更好，为了排除损失函数和优化器的影响，对于CNN模型也采用同样的loss和优化器，看一下变化**

- **<font color="green">得出的结论是，调参要慢慢来，多试试各种损失函数，各种优化器，之后还要对于不同模型使用！而且需要注意，训练时的batch_size不同，epoch不同，对于不同损失函数和优化器，效果来得快慢也是不同的</font>**
- **接着采用长时间训练，采用mean_absolute_error，adam作为优化器，采用后面写的6层深度模型进行训练，300次过后，test上的误差为0.46，而在train上的误差是0.38。接着又来300次，train上的误差为0.37，而test上的误差已经达到了0.51了，也就是说，过拟合实际上很早之间就已经发生了！无论怎样，都很难将test的loss降到0.45以下**
- keras中可以自定义loss函数，接收pred和true这两个节点，返回一个float，因而每次求取loss，我都加上了进行predict后得到的loss。<font color="red">这样的方法很重要，还可以用在二分类对于不同混淆矩阵给予不同的权重（比如0判成1是0.2的loss，1判成0是0.8的loss），由于loss是train和test的加和，所以这样训练很有意义。**可以有一种渐进学习法，首先loss全部为train的loss，之后加上test的loss，然后不断增加testloss的比例，减少trainloss的比例，直到几乎完全是testloss**</font>
- 终于，在改变loss之后的训练下，侧重于test的训练，train和test误差达到了0.41和0.43左右
- **存储模型结构，然后保存权重到文件夹，以日期命名，如果重复则后面加上1,2，然后备份py文件<font color="blue">从今往后，每次数据竞赛，提交的内容都需要存储，基本是存储*模型或权重、代码、提交文件*这三大件</font>**


---

- 2017年7月31日，提交的结果，其误差为0.44左右，这和训练时得到的误差几乎没有差别，也就表明，这很好的控制了过拟合，如果这个模型继续训练能够降低误差，那么最好，但是这个模型还能够提升吗？
- 原始数据集的量约为110万，因而我认为模型的参数量应该高一点儿，之前的模型参数量为14万左右，参数小了可能很难达到比较好的效果，另外**对激活函数调参，也是需要的**

### 突然想到一个问题，用前两个小时的图像去预测后一个小时的，是不是太牵强了？我还是应该比如5帧，也就是10分钟做一个视频，前后视频间隔5帧，用上一个视频去预测下一个视频；然后，重要的来了，还可以设计用20帧视频去预测下一个5帧的视频，然后将结果加和，得到后面5帧的视频，这样同时处理训练集和测试集，将其按照60帧，30帧，20帧，10帧和5帧得到x的1,2,3,6,12个视频，作为输入，求得下一个5帧的视频。而这里面的神经网络，还是采用ConvLSTM进行。
### 如果这样效果欠佳的话，就加上以前不同时段的数据集
- 2017年7月31日，今天采用了另一种方式，用早上的3个小时的数据！用第一个小时和第二个小时训练，这样可以训练3组，train2组，test1组，shape都是None 30 132，inout和output一样
- **除此之外，也可以采用所有的全部数据的30帧30帧来训练，但是这样反而不如直捣黄龙这种用前一小时预测后一小时这种训练更好，因为后者对于数据有分化倾向，十分投机取巧，是直接用于专门的预测的，其实是大数据提供方不希望的。那么如果只用被预测的那一个小时的前一个小时而不是前两个小时作为数据源，这样又如何如何呢？**
- 之后采用了前30帧预测后30帧的方法，最终得到的相对误差是0.7，这就不由得让人思考：**难道说因为加上了第一个30帧预测第二个30帧，使得模型普适性增加，导致不能够有更强的专一性，从而让7点预测8点的数据得不到过拟合的buff，从而增加了误差吗？如果是这样的话，我觉得一开始就应该*直接用前一个小时的数据预测后一个小时的数据，而不是用前两个小时预测后两个小时，这样能够直捣黄龙！训练出的模型就单单只用于7点到8点的预测，这样十分精准！***
- 但是又有一个问题，我用来预测的验证集，和之前采用60-30维的convLSTM的验证集是一样的，只不过x_test一个是60 132，一个是30 132，但是之前预测的，绝对误差大于本次预测的，而相对误差又小于本次预测的，**这表明这个模型可能在真实数值较大时误差小，真实数值小的时候误差大，为什么会有如此大的差异？**
- 之前的模型可能存在参数不足，导致最后优化很慢的现象，因而主要的改动：用30 to 30 代替60 to 30，不要第一个小时的数据；增加参数数量。代码上的改动，基本上就只有x_train进行一次切片，切取30：的数据，然后模型input参数改变，后边的2步长改为1步长
- **结果新来的模型一开始就只有0.44的误差，12的绝对误差**但是为了能够在未来训练有潜力，这里将模型参数扩大
- **dropout的高比例保证了稀疏性，因而这个模型应当按照*稀疏自编码器*的结构进行设计，中间参数不断增加，到最后不断减少，不要出现一会上升一会儿下降的现象**
- 于是这样设计网络：filter先增后减（对应于不同的LSTM时间尺度），所有kernel都是132的第一维度shape，除了最后一维是CONV3D，其他都是CONV2DLSTM
- 问题在于，无论怎样训练，似乎就像是有一个门槛，误差在0.45就难以下降了，先尝试一下激活函数调参吧！
- 之前一直全部使用tanh作为激活函数，得到的误差总是在0.45左右。现在全部采用relu：误差上升到1.9；接着又换回tanh：误差又降回0.44；接着第一层采用sigmoid，无太大变化，第一层和最后一层都采用sigmoid，误差上升到0.8（恐怕最后一层激活函数很重要）。由于找不到selu函数，从官网上找到selu的源代码，加进去，使用selu作为激活函数，可能效果和tanh差不多。**非常需要注意的是，LSTM中的return_sequences的激活函数不要随意更改，容易引起loss=nan**之后取消batch_normalization，全部使用selu，训练结果还是0.45左右。
- 接着采用8层模型，训练出来误差还是一开始就是0.44，之后将卷积核由132变成6，竟然还是差不多！
- 于是我尝试了2层模型，只有两层！结果loss为0.5，只上升了一点点，并且随着训练会下降，很快训练完之后，竟然这个两层的模型也达到了0.45的误差！
- <font color="red">后来惊讶地发现，上面的一个2层模型，对于不同的x，其预测结果竟然非常相似！就像是先有了一个平均值，然后在这个平均值上下波动一样！这是不是限制预测准确性的因素呢？
- 同样，我也发现，将train数据集改变后，比如用前1个小时的数据而不是前2个小时的数据，得到的预测结果，也是极其相似的！而且。看了看之前上传的数据，同样也是这样：几乎每一个每天的时间序列预测的结果，都十分相似。考虑到训练之后很快就得到了0.45的误差，推测可能模型仅仅只是取了一下平均什么的！但是模型的能力是可以使结果有很大变动的，所以我认为，模型训练，还远远不足！
- 不考虑其他的，我决定继续训练之前提交得到0.45误差的模型，希望它能够有所改观。采用selu作为激活函数，然后dropout设置为0.7
</font>
- keras中可以添加高级的激活函数，比如keras.layers.advanced_activations.LeakyReLU(alpha=0.3)。
- 令人惊讶的是，y_为各种各样的值，小到40，大到200时，几乎所有预测出来的值y，都是61左右，这样变动太小，就像是投机取巧找到了一个平均值一样，根本没有达到预测的目的！


- 之后尝试深度模型，现在模型是4层，之后准备加到8层





### 可以采用CNN+CONVLSTM的方法，另一个维度用图像处理，但是似乎也没有必要

## 尝试集成学习：如采用两种方法，结果根据两种方法取加权均值

---
- 这次我采用了1的batch_size，然后对之前的所有涉及过的模型进行了训练，观察到底哪个比较合适。
- 采用8层网络训练的时候，最终loss基本上停留在0.68左右，但是误差仍然停留在0.45左右
- **可以在最后sum的时候加上一个axis-0，这样可以保留第一维度，就能够看到对于92个或者30个样本，每个样本的情况如何**。结果还是出现了相同的现象：预测结果几乎是一致的
- 即使对数据取部分结果，还是一样的，容易陷入困境
- <font color='red' size=5>然而，当使用卷积网络而不是CONVLSTM的时候，结果就是差异化的！不是全部都是61左右的结果，而是有浮动，虽然误差很大，但是这个模型至少是可以训练的！不过在长时间训练过后，总的值总是会倾向于61这个值</font>
- **在长时间的训练之后，CNN还是能够达到0.45的准确率，既然如此，就没有必要使用ConvLSTM了。但是经过长时间的训练之后，最终得到的结果，还是很多61的值，大概ConvLSTM要比CNN收敛到这个值更快吧**
- **既然如此，就不能不说这存在很严重的过拟合倾向了，模型的泛化能力很低，所以很有必要把之前删掉的数据找回来再训练！用前一个小时的数据预测后一个小时的数据，如此进行训练！**
- 开始训练，从92个样本拓展到了2206个样本，采用8层ConvLSTM
- 实验结果是：误差很大。
- **最新的实验结果表明，经过很多次训练之后，得到的数据，竟然还是一模一样的！对于每个道路的相同frame，预测结果都是相同的！为什么会这样？是因为数据变动太小了吗**
- 针对输出一样的原因进行百度，可能的原因，是某个输出值太大，导致激活函数“平坦化”，所以在激活函数之前要进行正则化
- 实际观察到，一开始的训练，预测值是各有差异，之后随着训练的loss减少，预测值差异减小！**可能我的loss函数有问题，如果用平方的话，虽然误差可能增大，但是预测结果就不会趋于一致了！但结果显示，仍然是这样：预测结果还是很没有差异性**




## 新方法1
采用133的矩阵作为输入，即132+时间标志，预测后一个时间段的132输出，为133-132

## 新方法2
- 采用132作为输入，30 x 132 输出，其中含有30层全连接层，每层有一个132维的输出，每层输出都和y_train求取loss！
- 输入为None，1，132，输出为None，30，132，其中倒数第二层是30层dense，最后一层是concat按照axs=1连接所有层。
- **训练很快，经过200,10，batch_szie=1的训练后，终于，答案再也不是统一的61，虽然有一部分会集群有61，但是其他部分有大有小，为了提升这种差异性，我将loss函数设置为误差的4次方！**
- 最终其实可以看出，在每一个batch中，loss的差异很大，这就保证了为了让loss下降，神经网络必须适应全局





---
















## 摘要
提供一种无需人工建模进行交通量预测的方法，将每一时刻（每一帧）的道路流量信息转化为132长度的矩阵，将132长度的矩阵当做图片处理，利用ConvLSTM深度学习方法进行预测，经过小样本数据训练之后能够达到MAPE值在0.44左右。
## 数据处理
1. 在这个模型中，存储节点和路宽两个信息的文件都是不需要的，只需要gy_contest_link_traveltime_training_data.txt这个文件。将文件中的时间+日期转化为int格式，道路流量信息按照日期和时间进行划分，由于每段起始时间和结束时间相差2min，直接使用起始时间。
2. 经过步骤1，得到了shape为68002 x 132的数据，可以把132的数据当作一个时间帧下的一幅132 x 1 x 1（通道数）的图片，于是整个数据就相当于68002帧的视频，按照处理视频的思路用ConvLSTM处理
3. 由于计算设备的原因，我筛掉了87%的数据，只剩下3月到5月每天6-9点以及6月每天6-8点的数据，将3-5月6-8点的数据（shape为92*60， 132）作为X，3-5月8-9点的数据（shape为92*30，132）作为Y，经过0.25测试集的划分得到x_train,y_train,x_test,y_test数据。
4. 数据reshape为(92, 60, 132, 1, 1)或者(92, 60, 132, 1, 1)，表明训练集有92个60帧的视频，每个视频132 x 1大小，测试集的视频为30帧。
5. 这只是一种处理方式，也可以选择将所有原始的数据集reshape为(None, 30, 132, 1, 1)，每个视频是1个小时（30帧），然后神经网络输入是前1小时视频，输出是后一个小时视频，模型形状上像是一个ConvLSTM稀疏自编码器。
6. 另一种处理方式是用卷积，将数据变为(None, 30, 132, 1)，此时数据就是很多张30 x 132的图片，用上一张图片预测下一张图片。
## 模型建立
1. 模型需要将(None, 60, 132, 1,1)的输入经过神经网络变换成(None, 30, 132, 1, 1)的输出。
2. 推荐采用keras中已有的ConvLSTM层，再利用Conv3D，设置步长为2将第二维由60压缩为30
```python
model = Sequential()
model.add(ConvLSTM2D( filters=5,
	input_shape=(60, 132, 1 ,1),
	kernel_size=(132,1),
	padding="same"),activation="selu")
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Conv3D(filters=5,
	kernel_size=(132,1,2),
	strides=(2,1,1),padding="same"),
		activation="selu")
```
3. loss采用mean_squared_error，优化器采用adadelta。
## 结果分析
由于训练样本较少，所以模型很快收敛，最终得到的MAPE值在0.44左右
## 一些看法、理解和注意事项
1. 之所以使用ConvLSTM，是因为每一帧“图片”前后是时间序列化的，可以很好的模拟。
2. 卷积核采用了较大的面积，是因为132个道路的流量之间存在相关关系，但是这个相关关系可能隔得很远，（比如第1个位置和第132个位置可能有相关性，这和图像相近空间上有相关性而采用较小的卷积核是不同的），所以卷积核大一些，另一种方式是卷积更深一些，也能够将间隔远的节点联系起来
3. 本次训练中我采用的样本数量只有92个，因而训练很快收敛之后，MAPE就难以继续下降了，第一次提交的时候训练集上MAPE只有0.3，但提交上的测试集MAPE为0.6，严重过拟合，之后增加dropout过后，训练集上的MAPE一直下不去
4. 
